{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "- [OPUS](https://opus.nlpl.eu/)\n",
    "- [WMT](https://www.statmt.org/wmt20/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('imgs/')\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_unpack(url, output_path):\n",
    "    \"\"\" Скачивание файла и его распаковка. \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download completed.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Failed to download file.\")\n",
    "        return False\n",
    "\n",
    "def extract_gz(file_path, extract_to):\n",
    "    \"\"\" Распаковка gz архива. \"\"\"\n",
    "    with gzip.open(file_path, 'rb') as f_in:\n",
    "        with open(extract_to, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "def read_tmx_to_dataframe(tmx_path):\n",
    "    \"\"\" Чтение TMX файла и преобразование в DataFrame. \"\"\"\n",
    "    tree = ET.parse(tmx_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    data = []\n",
    "    for tu in root.findall('.//tu'):\n",
    "        tuv = tu.findall('tuv')\n",
    "        if len(tuv) >= 2:\n",
    "            src_text = tuv[0].find('seg').text\n",
    "            trg_text = tuv[1].find('seg').text\n",
    "            data.append({'source': src_text, 'target': trg_text})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Пример использования:\n",
    "# url = 'https://object.pouta.csc.fi/OPUS-wikimedia/v20230407/tmx/en-ru.tmx.gz'  # Замените на вашу ссылку\n",
    "\n",
    "# # Укажите URL архива и локальные пути для сохранения\n",
    "# download_path = os.path.join(DATA_PATH, 'en-ru.tmx.gz')\n",
    "# tmx_path = os.path.join(DATA_PATH, 'en-ru.tmx')\n",
    "\n",
    "# if download_and_unpack(url, download_path):\n",
    "#     extract_gz(download_path, tmx_path)\n",
    "#     dataframe = read_tmx_to_dataframe(tmx_path)\n",
    "#     print(dataframe.head())\n",
    "\n",
    "#     # Опционально: удаление временных файлов\n",
    "#     # os.remove(download_path)\n",
    "#     # os.remove(tmx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmx_path = os.path.join(DATA_PATH, 'en-ru.tmx')\n",
    "dataframe = read_tmx_to_dataframe(tmx_path)\n",
    "dataframe = dataframe[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка данных от пустых строк и None\n",
    "dataframe = dataframe.dropna()  # Удаление строк, где есть хотя бы один None\n",
    "dataframe = dataframe[dataframe['source'].str.strip().astype(bool) & dataframe['target'].str.strip().astype(bool)]  # Удаление пустых строк\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет\n",
    "## Токенизация и создание словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Определение класса TranslationDataProcessor\n",
    "class TranslationDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataframe, \n",
    "                 source_field, \n",
    "                 target_field, \n",
    "                 src_lang, \n",
    "                 trg_lang, \n",
    "                 max_vocab_size=10000, \n",
    "                 min_freq=2, \n",
    "                 max_length=50,\n",
    "                 test_size=0.1, \n",
    "                 batch_size=32):\n",
    "        self.dataframe = dataframe\n",
    "        self.source_field = source_field\n",
    "        self.target_field = target_field\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "        # Загрузка моделей spacy для токенизации\n",
    "        self.tokenizer_src = spacy.load(src_lang).tokenizer\n",
    "        self.tokenizer_trg = spacy.load(trg_lang).tokenizer\n",
    "        \n",
    "        # Разделение данных на обучающие и тестовые наборы\n",
    "        train_df, val_df = train_test_split(dataframe, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Построение словарей\n",
    "        self.src_vocab = self.build_vocab(self.tokenize_src, self.dataframe[source_field], max_vocab_size, min_freq)\n",
    "        self.trg_vocab = self.build_vocab(self.tokenize_trg, self.dataframe[target_field], max_vocab_size, min_freq)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Создание и сохранение обучающего и тестового датасетов\n",
    "        self.train_dataset = TranslationDataset(\n",
    "            train_df, \n",
    "            source_field, \n",
    "            target_field, \n",
    "            self.tokenize_src, \n",
    "            self.tokenize_trg, \n",
    "            self.src_vocab, \n",
    "            self.trg_vocab, \n",
    "            max_length\n",
    "        )\n",
    "        self.val_dataset = TranslationDataset(\n",
    "            val_df, \n",
    "            source_field, \n",
    "            target_field, \n",
    "            self.tokenize_src, \n",
    "            self.tokenize_trg, \n",
    "            self.src_vocab, \n",
    "            self.trg_vocab, \n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        # Создание загрузчиков данных для обучения и валидации\n",
    "        self.train_dataloader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=self.collate_fn, \n",
    "            shuffle=True\n",
    "        )\n",
    "        self.val_dataloader = DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            collate_fn=self.collate_fn, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def tokenize_src(self, text):\n",
    "        return [tok.text.lower() for tok in self.tokenizer_src(text)]\n",
    "\n",
    "    def tokenize_trg(self, text):\n",
    "        return [tok.text.lower() for tok in self.tokenizer_trg(text)]\n",
    "\n",
    "    def build_vocab(self, tokenizer, texts, max_size, min_freq, specials=['<unk>', '<pad>', '<bos>', '<eos>']):\n",
    "        # Создаем итератор, который применяет токенизатор к каждому предложению\n",
    "        def token_generator():\n",
    "            for sentence in texts:\n",
    "                yield tokenizer(sentence)\n",
    "\n",
    "        # Использование build_vocab_from_iterator для создания словаря\n",
    "        vocab_obj = build_vocab_from_iterator(token_generator(), specials=specials, max_tokens=max_size, min_freq=min_freq)\n",
    "        \n",
    "        # Установка индекса для неизвестного токена\n",
    "        vocab_obj.set_default_index(vocab_obj['<unk>'])\n",
    "\n",
    "        return vocab_obj\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        src_batch = [item['src'] for item in batch]\n",
    "        trg_batch = [item['trg'] for item in batch]\n",
    "        src_padded = pad_sequence(src_batch, padding_value=self.src_vocab['<pad>'], batch_first=True)\n",
    "        trg_padded = pad_sequence(trg_batch, padding_value=self.trg_vocab['<pad>'], batch_first=True)\n",
    "        return {\"src\": src_padded, \"trg\": trg_padded}\n",
    "\n",
    "# Определение класса TranslationDataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, source_field, target_field, src_tokenizer, trg_tokenizer, src_vocab, trg_vocab, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.source = [self.encode_sentence(sentence, self.src_vocab, self.src_tokenizer) for sentence in dataframe[source_field]]\n",
    "        self.target = [self.encode_sentence(sentence, self.trg_vocab, self.trg_tokenizer) for sentence in dataframe[target_field]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": torch.tensor(self.source[idx], dtype=torch.long),\n",
    "            \"trg\": torch.tensor(self.target[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def encode_sentence(self, sentence, lang_vocab, tokenizer):\n",
    "        tokenized_text = tokenizer(sentence)\n",
    "        numericalized_text = [lang_vocab['<bos>']] + [lang_vocab[token] if token in lang_vocab else lang_vocab['<unk>'] for token in tokenized_text] + [lang_vocab['<eos>']]\n",
    "        return numericalized_text[:self.max_length]\n",
    "    \n",
    "    def decode_sentence(self, numericalized_sentence, vocab: Vocab):\n",
    "        \"\"\"\n",
    "        Декодирует числовую последовательность обратно в текст, исключая специальные символы.\n",
    "\n",
    "        Args:\n",
    "            numericalized_sentence (List[int]): Список числовых индексов слов.\n",
    "            vocab (Vocab): Словарь для декодирования индексов.\n",
    "\n",
    "        Returns:\n",
    "            str: Декодированное предложение.\n",
    "        \"\"\"\n",
    "        special_tokens = {vocab['<bos>'], vocab['<eos>'], vocab['<pad>'], vocab['<unk>']}\n",
    "        decoded_sentence = [vocab.get_itos()[index] for index in numericalized_sentence if index not in special_tokens]\n",
    "        return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Загрузка моделей SpaCy для токенизации\n",
    "src_lang_model = 'en_core_web_sm'  # Модель для английского языка\n",
    "trg_lang_model = 'ru_core_news_sm'  # Модель для русского языка\n",
    "\n",
    "# Использование класса TranslationDataProcessor\n",
    "processor = TranslationDataProcessor(dataframe, 'source', 'target', src_lang_model, trg_lang_model, max_length=100)\n",
    "\n",
    "# Получение загрузчика данных\n",
    "# processor.train_dataset.source[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_indices = [2, 10, 7, 17, 23, 4]  # предположим, это выход модели\n",
    "decoded_output = processor.train_dataset.decode_sentence(example_output_indices, processor.train_dataset.trg_vocab)\n",
    "print(\"Декодированное предложение:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = processor.train_dataloader\n",
    "\n",
    "for batch in loader:\n",
    "    src = batch[\"src\"]\n",
    "    trg = batch[\"trg\"]\n",
    "    print('*'*80)\n",
    "    print(src.shape)\n",
    "    print(trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    src_vocab_size = 32000  # Предполагаем, что у нас есть 32K уникальных токенов для исходного языка\n",
    "    trg_vocab_size = 32000  # и для целевого языка\n",
    "    src_pad_idx = 0         # Индекс паддинга для исходного языка\n",
    "    trg_pad_idx = 0         # Индекс паддинга для целевого языка\n",
    "    embed_size = 512        # Размер эмбеддингов\n",
    "    num_layers = 6          # Количество слоев в энкодере и декодере\n",
    "    forward_expansion = 4   # Коэффициент увеличения для полносвязного слоя\n",
    "    heads = 8               # Количество голов в механизме многослойного внимания\n",
    "    dropout = 0.1           # Вероятность dropout\n",
    "    max_length = 100        # Максимальная длина последовательности\n",
    "    device = \"cuda\"         # Устройство для тренировки: 'cuda' или 'cpu'\n",
    "    learning_rate = 0.0005  # Скорость обучения\n",
    "    batch_size = 64         # Размер батча для обучения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для реализации механизма Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout_rate=0.1, scale_factor=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size  # Размерность эмбеддинга\n",
    "        self.heads = heads  # Количество attention heads\n",
    "        self.head_dim = embed_size // heads  # Размерность для каждой головы внимания\n",
    "        self.scale_factor = scale_factor if scale_factor is not None \\\n",
    "            else (self.embed_size ** 0.5)\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size должен быть кратен количеству голов\"\n",
    "\n",
    "        # Инициализация весов для query, key и value\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # Объединение результатов голов внимания\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        # Получение размера пакета\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Разбиваем входные тензоры на головы\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # Применяем линейные преобразования к значениям\n",
    "        keys = self.keys(keys)  # Применяем линейные преобразования к ключам\n",
    "        queries = self.queries(queries)  # Применяем линейные преобразования к запросам\n",
    "\n",
    "        # Multiplying queries and keys for attention scores (N, heads, query_len, key_len)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalizing the attention scores using softmax\n",
    "        attention = torch.softmax(energy / self.scale_factor, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Multiplying the attention scores with the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            batch_size, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "                \n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для одного блока Transformer, включающий в себя Self-Attention и Feed Forward сеть.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)  # Создаем слой Self-Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Слой нормализации для residual connection после attention\n",
    "        self.norm2 = nn.LayerNorm(embed_size)  # Слой нормализации для residual connection после feed forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),  # Линейный слой\n",
    "            nn.ReLU(),  # Функция активации ReLU\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Линейный слой\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Слой dropout для регуляризации\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через блок Transformer.\n",
    "\n",
    "        Аргументы:\n",
    "            value: Тензор значений размером (batch_size, seq_length, embed_size)\n",
    "            key: Тензор ключей размером (batch_size, seq_length, embed_size)\n",
    "            query: Тензор запросов размером (batch_size, seq_length, embed_size)\n",
    "            mask: Маска внимания для исключения паддинга, размером (batch_size, 1, seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через блок Transformer\n",
    "        \"\"\"\n",
    "        # Проходим через механизм Self-Attention\n",
    "        attention_out, _ = self.attention(value, key, query, mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        x = self.dropout(self.norm1(attention_out + query))\n",
    "        # Проходим через feed forward сеть\n",
    "        forward_out = self.feed_forward(x)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        out = self.dropout(self.norm2(forward_out + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для Encoder части Transformer, содержащий стек Transformer блоков.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 embed_size, \n",
    "                 num_layers, \n",
    "                 heads, \n",
    "                 device, \n",
    "                 forward_expansion, \n",
    "                 dropout, \n",
    "                 max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # Создаем эмбеддинги слов\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)  # Создаем позиционные эмбеддинги\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size, \n",
    "                    heads, \n",
    "                    dropout, \n",
    "                    forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )  # Создаем стек блоков Transformer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через стек блоков Transformer Encoder.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, src_seq_length)\n",
    "            mask: Маска внимания для исключения паддинга, размером (batch_size, 1, src_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через стек блоков Transformer Encoder\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)  # Генерируем позиции\n",
    "        \n",
    "        # Получаем эмбеддинги слов и позиций и суммируем их\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Проходим через стек блоков Transformer\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для блока Decoder, который включает в себя Self-Attention, Encoder-Decoder Attention и Feed Forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)  # Создаем слой Self-Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.encoder_decoder_attention = SelfAttention(embed_size, heads)  # Создаем слой внимания между Decoder и Encoder\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),  # Линейный слой\n",
    "            nn.ReLU(),  # Функция активации ReLU\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Линейный слой\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Слой dropout для регуляризации\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через блок Decoder.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, trg_seq_length, embed_size)\n",
    "            value: Тензор значений размером (batch_size, src_seq_length, embed_size)\n",
    "            key: Тензор ключей размером (batch_size, src_seq_length, embed_size)\n",
    "            src_mask: Маска внимания для исключения паддинга в Encoder, размером (batch_size, 1, src_seq_length)\n",
    "            trg_mask: Маска внимания для исключения предсказаний из будущего, размером (batch_size, trg_seq_length, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через блок Decoder\n",
    "        \"\"\"\n",
    "        # Проходим через механизм Self-Attention внутри Decoder\n",
    "        attention_out, _ = self.attention(x, x, x, trg_mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        query = self.dropout(self.norm1(attention_out + x))\n",
    "        \n",
    "        # Проходим через механизм внимания между Decoder и Encoder\n",
    "        encoder_decoder_attention_out, _ = self.encoder_decoder_attention(value, key, query, src_mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        query = self.dropout(self.norm2(encoder_decoder_attention_out + query))\n",
    "\n",
    "        # Проходим через feed forward сеть\n",
    "        forward_out = self.feed_forward(query)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        out = self.dropout(self.norm3(forward_out + query))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для Decoder части Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        trg_vocab_size, \n",
    "        embed_size, \n",
    "        num_layers, \n",
    "        heads, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        device, \n",
    "        max_length):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)  # Создаем эмбеддинги слов\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)  # Создаем позиционные эмбеддинги\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "            for _ in range(num_layers)\n",
    "        ])  # Создаем стек блоков Decoder\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)  # Финальный линейный слой для предсказания слов\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через Decoder модель Transformer.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, trg_seq_length)\n",
    "            enc_out: Выходные данные Encoder размером (batch_size, src_seq_length, embed_size)\n",
    "            src_mask: Маска внимания для исключения паддинга в Encoder, размером (batch_size, 1, src_seq_length)\n",
    "            trg_mask: Маска внимания для исключения предсказаний из будущего, размером (batch_size, trg_seq_length, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через Decoder модель Transformer\n",
    "        \"\"\"\n",
    "        batch_size, trg_seq_length = x.shape\n",
    "        positions = torch.arange(0, trg_seq_length).expand(batch_size, trg_seq_length).to(self.device)  # Генерируем позиции\n",
    "\n",
    "        # Получаем эмбеддинги слов и позиций и суммируем их\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        # Проходим через стек блоков Decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)  # Применяем линейный слой для предсказания следующего слова\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс, интегрирующий Encoder и Decoder в полную модель Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 trg_vocab_size, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 embed_size=256, \n",
    "                 num_layers=6, \n",
    "                 forward_expansion=4, \n",
    "                 heads=8, \n",
    "                 dropout=0, \n",
    "                 device=\"cuda\", \n",
    "                 max_length=100):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length\n",
    "        )\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        Создание маски для исключения влияния padding токенов в процессе внимания.\n",
    "\n",
    "        Аргументы:\n",
    "            src: Тензор исходных данных размером (batch_size, src_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            src_mask: Маска для исключения padding токенов размером (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (batch_size, 1, 1, src_len) для работы с механизмом внимания\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.size()\n",
    "        no_peak_mask = torch.tril(torch.ones((1, trg_len, trg_len), device=self.device)).bool()\n",
    "        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_mask = no_peak_mask & pad_mask\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Полный проход через модель.\n",
    "\n",
    "        Аргументы:\n",
    "            src: Тензор исходных данных размером (batch_size, src_seq_length)\n",
    "            trg: Тензор целевых данных размером (batch_size, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            output: Результат предсказания модели размером (batch_size, trg_seq_length, trg_vocab_size)\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from rouge import Rouge\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# class Evaluator:\n",
    "#     def __init__(self, \n",
    "#                  tokenizer_src: Tokenizer, \n",
    "#                  tokenizer_trg: Tokenizer,\n",
    "#                  weights={'bleu': 0.34, 'rouge': 0.33, 'meteor': 0.33}):\n",
    "#         \"\"\"\n",
    "#         Инициализация Evaluator с токенизаторами для исходного и целевого языков.\n",
    "\n",
    "#         :param tokenizer_src: Токенизатор для исходного языка.\n",
    "#         :param tokenizer_trg: Токенизатор для целевого языка.\n",
    "#         :param weights: Веса.\n",
    "#         \"\"\"\n",
    "#         self.tokenizer_src = tokenizer_src\n",
    "#         self.tokenizer_trg = tokenizer_trg\n",
    "#         self.weights = weights\n",
    "        \n",
    "#         self.rouge = Rouge()\n",
    "\n",
    "#     def evaluate(self, model, dataloader):\n",
    "#         \"\"\"\n",
    "#         Оценивает модель на данных, загруженных через data_loader.\n",
    "\n",
    "#         :param model: Модель для оценки.\n",
    "#         :param data_loader: DataLoader, предоставляющий данные для оценки.\n",
    "#         :return: Словарь с результатами по метрикам.\n",
    "#         \"\"\"\n",
    "#         model.eval()\n",
    "#         references = []\n",
    "#         hypotheses = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in dataloader:\n",
    "#                 src  = batch['src_texts'].to(model.device)\n",
    "#                 trg  = batch['trg_texts'].to(model.device)\n",
    "\n",
    "#                 # Генерация вывода модели\n",
    "#                 output = model(src, trg[:, :-1])  # Исключаем токен <eos>\n",
    "#                 output = output.argmax(-1)\n",
    "                \n",
    "#                 # Детокенизация результата\n",
    "#                 for true, pred in zip(trg, output):\n",
    "#                     true_sentence = self.tokenizer_trg.detokenize(true.cpu().numpy())\n",
    "#                     pred_sentence = self.tokenizer_trg.detokenize(pred.cpu().numpy())\n",
    "#                     references.append([true_sentence])\n",
    "#                     hypotheses.append(pred_sentence)\n",
    "\n",
    "#         # Вычисление метрик BLEU, ROUGE, METEOR\n",
    "#         bleu_score = corpus_bleu(references, hypotheses)\n",
    "#         rouge_score = self.rouge.get_scores(hypotheses, references, avg=True)['f']\n",
    "#         list_meteor_score = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "#         avg_meteor_score = np.mean(list_meteor_score)\n",
    "\n",
    "#         # Словарь с результатами\n",
    "#         results = {\n",
    "#             'overall': self.weights['bleu'] * bleu_score +\n",
    "#                        self.weights['rouge'] * rouge_score +\n",
    "#                        self.weights['meteor'] * avg_meteor_score,\n",
    "#             'bleu': bleu_score,\n",
    "#             'rouge': rouge_score,\n",
    "#             'meteor': avg_meteor_score\n",
    "#         }\n",
    "#         return results\n",
    "    \n",
    "#     def evaluate_on_indices(self, model, dataloader):\n",
    "#         \"\"\"\n",
    "#         Оценивает модель на данных, используя индексы токенов для вычисления BLEU.\n",
    "\n",
    "#         :param model: Модель для оценки.\n",
    "#         :param data_loader: DataLoader, предоставляющий данные для оценки.\n",
    "#         :return: Словарь с результатами метрики BLEU.\n",
    "#         \"\"\"\n",
    "#         model.eval()\n",
    "#         references = []\n",
    "#         hypotheses = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in dataloader:\n",
    "#                 src  = batch['src_texts'].to(model.device)\n",
    "#                 trg  = batch['trg_texts'].to(model.device)\n",
    "                \n",
    "#                 # Генерация вывода модели\n",
    "#                 output = model(src, trg[:, :-1])\n",
    "#                 output = output.argmax(-1)\n",
    "                \n",
    "#                 # Сохранение индексов без детокенизации\n",
    "#                 for true, pred in zip(trg, output):\n",
    "#                     # Убираем индексы для специальных токенов\n",
    "#                     true_indices = [token for token in true.cpu().numpy() if token not in (self.tokenizer_trg.vocab['<bos>'], self.tokenizer_trg.vocab['<eos>'], self.tokenizer_trg.vocab['<pad>'])]\n",
    "#                     pred_indices = [token for token in pred.cpu().numpy() if token not in (self.tokenizer_trg.vocab['<bos>'], self.tokenizer_trg.vocab['<eos>'], self.tokenizer_trg.vocab['<pad>'])]\n",
    "                    \n",
    "#                     references.append([true_indices])\n",
    "#                     hypotheses.append(pred_indices)\n",
    "\n",
    "#         # Вычисление метрик BLEU, ROUGE, METEOR\n",
    "#         bleu_score = corpus_bleu(references, hypotheses)\n",
    "#         rouge_score = self.rouge.get_scores(hypotheses, references, avg=True)['f']\n",
    "#         list_meteor_score = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "#         avg_meteor_score = np.mean(list_meteor_score)\n",
    "\n",
    "#         # Словарь с результатами\n",
    "#         results = {\n",
    "#             'overall': self.weights['bleu'] * bleu_score +\n",
    "#                        self.weights['rouge'] * rouge_score +\n",
    "#                        self.weights['meteor'] * avg_meteor_score,\n",
    "#             'bleu': bleu_score,\n",
    "#             'rouge': rouge_score,\n",
    "#             'meteor': avg_meteor_score\n",
    "#         }\n",
    "#         return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# class Trainer:\n",
    "#     def __init__(self, \n",
    "#                  model:Transformer, \n",
    "#                  evaluator: Evaluator, \n",
    "#                  optimizer:nn.CrossEntropyLoss,\n",
    "#                  criterion:optim.Adam,\n",
    "#                  device='cuda',\n",
    "#                  printing_period=None,\n",
    "#                  imgs_path=Path(''),\n",
    "#                  ):\n",
    "#         \"\"\"\n",
    "#         Инициализация Trainer для модели Transformer.\n",
    "#         \"\"\"\n",
    "#         self.model = model.to(device)\n",
    "#         self.device = device\n",
    "#         self.evaluator = evaluator\n",
    "        \n",
    "#         self.printing_period = printing_period\n",
    "#         self.imgs_path = imgs_path\n",
    "#         self.train_losses = []\n",
    "#         self.val_losses = []\n",
    "#         self.metrics = []  # Словарь для хранения всех метрик\n",
    "\n",
    "#         self.optimizer = optimizer\n",
    "#         self.criterion = criterion\n",
    "\n",
    "#     def train(self, train_dataloader):\n",
    "#         \"\"\"\n",
    "#         Обучение модели\n",
    "#         \"\"\"\n",
    "#         self.model.train()\n",
    "#         train_loss = 0.0\n",
    "#         for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "#             src = batch['src_texts'].to(self.device)\n",
    "#             trg = batch['trg_texts'].to(self.device)\n",
    "            \n",
    "#             # print('src_texts',src.shape) \n",
    "#             # print('trg_texts',trg.shape) \n",
    "#             output = self.model(src, trg[:, :-1])\n",
    "#             output = output.reshape(-1, output.shape[2])\n",
    "#             trg = trg[:, 1:].reshape(-1)\n",
    "#             loss = self.criterion(output, trg)\n",
    "\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "            \n",
    "#             # Аккумулируем потери\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         # Вычисляем среднюю потерю за эпоху\n",
    "#         avg_loss = train_loss / len(train_dataloader)\n",
    "#         self.train_losses.append(avg_loss)\n",
    "#         return avg_loss\n",
    "\n",
    "#     def eval(self, val_dataloader):\n",
    "#         \"\"\"\n",
    "#         Валидация модели для оценки производительности на валидационном наборе данных.\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         validation_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "#                 src = batch['src_texts'].to(self.device)\n",
    "#                 trg = batch['trg_texts'].to(self.device)                \n",
    "#                 output = self.model(src, trg[:, :-1])\n",
    "#                 output = output.reshape(-1, output.shape[2])\n",
    "#                 trg = trg[:, 1:].reshape(-1)\n",
    "#                 loss = self.criterion(output, trg)\n",
    "#                 validation_loss += loss.item()\n",
    "                \n",
    "#         avg_loss = validation_loss / len(val_dataloader)\n",
    "#         self.val_losses.append(avg_loss)\n",
    "#         metric_scores = self.evaluator.evaluate(self.model, val_dataloader)\n",
    "#         self.metrics.append(metric_scores)\n",
    "#         return avg_loss, metric_scores\n",
    "        \n",
    "#     def fit(self, num_epochs, train_dataloader, val_dataloader):\n",
    "#         for epoch in range(num_epochs):\n",
    "#             epoch_loss = self.train(train_dataloader)\n",
    "#             val_epoch_loss, metric_scores = self.eval(val_dataloader)   \n",
    "                 \n",
    "#             if self.printing_period and (epoch + 1) % self.printing_period == 0:\n",
    "#                 print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Overall: {metric_scores[\"overall\"]:.4f}')\n",
    "                \n",
    "#         return val_epoch_loss, metric_scores\n",
    "\n",
    "#     def save_model(self, path):\n",
    "#         \"\"\"\n",
    "#         Сохранение обученной модели.\n",
    "\n",
    "#         Параметры:\n",
    "#             path: путь для сохранения модели.\n",
    "#         \"\"\"\n",
    "#         torch.save(self.model.state_dict(), path)\n",
    "\n",
    "#     def load_model(self, path):\n",
    "#         \"\"\"\n",
    "#         Загрузка модели из файла.\n",
    "\n",
    "#         Параметры:\n",
    "#             path: путь к файлу с сохраненной моделью.\n",
    "#         \"\"\"\n",
    "#         self.model.load_state_dict(torch.load(path))\n",
    "#         self.model.to(self.device)\n",
    "        \n",
    "#     def _save_fig(self, fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "#         path = self.imgs_path / f\"{fig_id}.{fig_extension}\"\n",
    "#         if tight_layout:\n",
    "#             plt.tight_layout()\n",
    "#         plt.savefig(path, format=fig_extension, dpi=resolution) \n",
    "\n",
    "#     def plot_metrics(self):\n",
    "#         epochs_range = range(1, len(self.val_losses) + 1)\n",
    "#         fig = plt.figure(figsize=(15, 6))  # Устанавливаем размер фигуры\n",
    "        \n",
    "#         ax11 = plt.subplot(2, 2, 1)\n",
    "#         ax11.plot(epochs_range, self.train_losses, label='Train Loss', color='tab:red')\n",
    "#         ax11.plot(epochs_range, self.val_losses, label='Validation Loss', color='tab:blue')\n",
    "#         ax11.set_title('Losses over Epochs')\n",
    "#         ax11.set_xlabel('Epochs')\n",
    "#         ax11.set_ylabel('Loss')\n",
    "#         # ax11.tick_params(axis='y', labelcolor='tab:red')\n",
    "#         ax11.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "#         ax11.legend(loc='upper right')\n",
    "\n",
    "#         ax12 = plt.subplot(2, 2, 2)\n",
    "#         ax12.plot(epochs_range, [m['overall'] for m in self.metrics], label='Overall Score', color='tab:green')\n",
    "#         ax12.set_title('Overall Evaluation Score')\n",
    "#         ax12.set_xlabel('Epochs')\n",
    "#         ax12.set_ylabel('Score')\n",
    "#         ax12.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "#         ax12.legend(loc='upper right')\n",
    "\n",
    "#         # Отдельные графики для каждой метрики\n",
    "#         ax21 = plt.subplot(2, 2, 3)\n",
    "#         ax21.plot(epochs_range, [m['bleu'] for m in self.metrics], label='BLEU Score', color='tab:yellow')\n",
    "#         ax21.plot(epochs_range, [m['rouge'] for m in self.metrics], label='ROUGE Score', color='tab:pink')\n",
    "#         ax21.plot(epochs_range, [m['meteor'] for m in self.metrics], label='METEOR Score', color='tab:brown')\n",
    "#         ax21.set_title('Individual Metrics')\n",
    "#         ax21.set_xlabel('Epochs')\n",
    "#         ax21.set_ylabel('Score')\n",
    "#         ax21.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "#         ax21.legend(loc='upper right')      \n",
    "\n",
    "#         fig.tight_layout()  # Убедимся, что макет не нарушен\n",
    "#         self._save_fig(\"train_metrics\")  # extra code\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainingManager:\n",
    "    def __init__(self, \n",
    "                 features: str, \n",
    "                 targets: str,\n",
    "                 dataset: pd.DataFrame,\n",
    "                 config: Config):\n",
    "        \"\"\"\n",
    "        Инициализация менеджера обучения.        \n",
    "        :param features: Признаки для обучения модели.\n",
    "        :param targets: Целевые значения.\n",
    "        :param model_params: Параметры модели, включая размеры слоёв и функцию активации.\n",
    "        :param train_params: Параметры обучения, включая размер батча, количество эпох и скорость обучения.\n",
    "        \"\"\"\n",
    "        self.uniq_name = '_'.join(features + targets)\n",
    "        self.data_path = DATA_PATH      \n",
    "        \n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.config = config        \n",
    "         \n",
    "    def _pred_job(self, dataset):\n",
    "        \n",
    "        # Обучение масштабировщика и трансформация признаков и целевой переменной\n",
    "        scaled_features = self.scaler_features.fit_transform(\n",
    "            dataset[self.features]\n",
    "        )\n",
    "        scaled_targets = self.scaler_targets.fit_transform(\n",
    "            dataset[self.targets]\n",
    "        )\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            scaled_features, \n",
    "            scaled_targets, \n",
    "            test_size=self.other_params.get('test_size', 0.8), \n",
    "            random_state=self.other_params.get('seed', 30), \n",
    "            shuffle=self.other_params.get('shuffle', False))        \n",
    "        \n",
    "        self.train_dataset = CustomDataset(X_train, y_train)\n",
    "        self.test_dataset = CustomDataset(X_test, y_test)\n",
    "        \n",
    "        model = RegressionModel(\n",
    "            input_size=len(self.features),\n",
    "            output_size=len(self.targets),\n",
    "            **self.model_params\n",
    "        )        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.train_params.get('learning_rate', 0.001))\n",
    "        criterion=torch.nn.MSELoss()\n",
    "        print_every=self.train_params.get('print_every', None)\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            # device='cpu',\n",
    "            print_every=print_every\n",
    "        )   \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Запуск процесса обучения.\n",
    "        \"\"\"\n",
    "        self._pred_job(self.dataset)\n",
    "        \n",
    "        batch_size = self.train_params.get('batch_size', 64)\n",
    "        n_epochs = self.train_params.get('n_epochs', 10)\n",
    "        \n",
    "        train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        self.trainer.fit(n_epochs, train_dataloader, val_dataloader)\n",
    "        self.trainer.plot_metrics()\n",
    "\n",
    "    def predict(self, input, transform=True):\n",
    "        \"\"\"\n",
    "        Выполнение предсказаний с помощью обученной модели.\n",
    "        \"\"\"\n",
    "        features = input\n",
    "        if transform:\n",
    "            features = self.scaler_features.transform(input)\n",
    "        else:\n",
    "            features = input\n",
    "        scaled_predictions = self.trainer.predict(features)\n",
    "        return self.scaler_targets.inverse_transform(scaled_predictions)\n",
    "\n",
    "    def save(self):\n",
    "        self._save_model()\n",
    "        self._save_standard_scalar()\n",
    " \n",
    "    def load(self):\n",
    "        self._load_model()\n",
    "        self._load_standard_scalar()\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"\n",
    "        Сохранение обученной модели.        \n",
    "        \"\"\"\n",
    "        self.trainer.save_model(self.data_path+self.uniq_name+'_model_weight.pth')\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Загрузка обученной модели.        \n",
    "        \"\"\"\n",
    "        self.trainer = Trainer.load_model(self.data_path+self.uniq_name+'_model_weight.pth')\n",
    "        \n",
    "    def _save_standard_scalar(self):\n",
    "        with open(self.data_path+self.uniq_name+'_scaler_features.pkl', 'wb') as file: \n",
    "            pickle.dump(self.scaler_features, file)\n",
    "        with open(self.data_path+self.uniq_name+'_scaler_targets.pkl', 'wb') as file: \n",
    "            pickle.dump(self.scaler_targets, file)\n",
    "        \n",
    "    def _load_standard_scalar(self):\n",
    "        with open(self.data_path+self.uniq_name+'_scaler_features.pkl', 'rb') as file: \n",
    "            self.scaler_features = pickle.load(file)\n",
    "        with open(self.data_path+self.uniq_name+'_scaler_targets.pkl', 'rb') as file: \n",
    "            self.scaler_targets = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config\n",
    "\n",
    "config.src_vocab_size = 32000  # Предполагаем, что у нас есть 32K уникальных токенов для исходного языка\n",
    "config.trg_vocab_size = 32000  # и для целевого языка\n",
    "config.src_pad_idx = 1         # Индекс паддинга для исходного языка\n",
    "config.trg_pad_idx = 1         # Индекс паддинга для целевого языка\n",
    "config.embed_size = 16        # Размер эмбеддингов\n",
    "config.num_layers = 2          # Количество слоев в энкодере и декодере\n",
    "config.forward_expansion = 4   # Коэффициент увеличения для полносвязного слоя\n",
    "config.heads = 2               # Количество голов в механизме многослойного внимания\n",
    "config.dropout = 0.1           # Вероятность dropout\n",
    "config.max_length = 100        # Максимальная длина последовательности\n",
    "config.device = \"cpu\"         # Устройство для тренировки: 'cuda' или 'cpu'\n",
    "config.learning_rate = 0.0005  # Скорость обучения\n",
    "config.batch_size = 2         # Размер батча для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Определение гиперпараметров\n",
    "max_vocab_size = 10000\n",
    "min_freq = 2\n",
    "max_length = 100\n",
    "embed_size = 256\n",
    "num_layers = 6\n",
    "forward_expansion = 4\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "clip = 1\n",
    "test_size = 0.1\n",
    "\n",
    "processor = TranslationDataProcessor(\n",
    "    dataframe=dataframe,\n",
    "    source_field='source',\n",
    "    target_field='target',\n",
    "    src_lang='en_core_web_sm',\n",
    "    trg_lang='ru_core_news_sm',\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    min_freq=min_freq,\n",
    "    max_length=max_length,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Получение загрузчиков данных для обучения и валидации\n",
    "train_dataloader = processor.train_dataloader\n",
    "val_dataloader = processor.val_dataloader\n",
    "\n",
    "# Определение модели\n",
    "src_vocab_size = len(processor.src_vocab)\n",
    "trg_vocab_size = len(processor.trg_vocab)\n",
    "\n",
    "# Шаг 1: Определение модели\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=processor.src_vocab['<pad>'],\n",
    "    trg_pad_idx=processor.trg_vocab['<pad>'],\n",
    "    embed_size=embed_size,\n",
    "    num_layers=num_layers,\n",
    "    forward_expansion=forward_expansion,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# Шаг 2: Определение функции потерь\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=processor.trg_vocab['<pad>'])\n",
    "\n",
    "# Шаг 3: Определение оптимизатора\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Шаг 4: Обучение модели\n",
    "def train(model: Transformer, iterator, optimizer: Adam, criterion: nn.CrossEntropyLoss, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator, desc=\"Training\"):\n",
    "        src = batch[\"src\"]\n",
    "        trg = batch[\"trg\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"Evaluation\"):\n",
    "            src = batch[\"src\"]\n",
    "            trg = batch[\"trg\"]\n",
    "\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    # valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     torch.save(model.state_dict(), 'transformer_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предполагается, что вы уже определили модель, критерий и оптимизатор, а также получили загрузчик данных\n",
    "\n",
    "# Определение функции обучения\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Переводим модель в режим обучения\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Цикл обучения\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            src_data, trg_data = batch['src'].to(device), batch['trg'].to(device)\n",
    "            \n",
    "            # Обнуляем градиенты параметров перед обратным распространением\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Проходим вперед через модель\n",
    "            output = model(src_data, trg_data[:, :-1])  # Передаем trg_data без последнего токена\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # Переводим размерности для вычисления функции потерь\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg_data = trg_data[:, 1:].contiguous().view(-1)  # Исключаем первый токен из trg_data\n",
    "            \n",
    "            # Вычисляем функцию потерь\n",
    "            loss = criterion(output, trg_data)\n",
    "            \n",
    "            # Обратное распространение и обновление параметров\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Вычисляем среднюю потерю на обучающем наборе данных\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Оценка модели на валидационном наборе данных\n",
    "        val_loss = evaluate_model(model, criterion, val_loader)\n",
    "        \n",
    "        # Выводим промежуточные результаты\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Определение функции оценки модели на валидационном наборе данных\n",
    "def evaluate_model(model, criterion, val_loader):\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Отключаем вычисление градиентов для ускорения\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            src_data, trg_data = batch['src'].to(device), batch['trg'].to(device)\n",
    "            \n",
    "            # Проходим вперед через модель\n",
    "            output = model(src_data, trg_data[:, :-1])  # Передаем trg_data без последнего токена\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # Переводим размерности для вычисления функции потерь\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg_data = trg_data[:, 1:].contiguous().view(-1)  # Исключаем первый токен из trg_data\n",
    "            \n",
    "            # Вычисляем функцию потерь\n",
    "            loss = criterion(output, trg_data)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Вычисляем среднюю потерю на валидационном наборе данных\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "# Параметры обучения\n",
    "num_epochs = 10\n",
    "\n",
    "# Обучение модели\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['src_padded'].values, dataframe['trg_padded'].values, test_size=0.2, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(X_train, y_train, config.max_length, config.src_pad_idx)\n",
    "val_dataset = TranslationDataset(X_test, y_test, config.max_length, config.src_pad_idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=config.trg_pad_idx)\n",
    "evaluator = Evaluator(tokenizer_en, tokenizer_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    evaluator=evaluator,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=config.device,\n",
    "    printing_period=1,\n",
    "    imgs_path=IMAGES_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(2, train_dataloader, val_dataloader)\n",
    "trainer.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выполнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
