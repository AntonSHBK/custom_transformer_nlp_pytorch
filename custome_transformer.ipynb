{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "- [OPUS](https://opus.nlpl.eu/)\n",
    "- [WMT](https://www.statmt.org/wmt20/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('imgs/')\n",
    "DATA_PATH = Path('data/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_unpack(url, output_path):\n",
    "    \"\"\" Скачивание файла и его распаковка. \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download completed.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Failed to download file.\")\n",
    "        return False\n",
    "\n",
    "def extract_gz(file_path, extract_to):\n",
    "    \"\"\" Распаковка gz архива. \"\"\"\n",
    "    with gzip.open(file_path, 'rb') as f_in:\n",
    "        with open(extract_to, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "def read_tmx_to_dataframe(tmx_path):\n",
    "    \"\"\" Чтение TMX файла и преобразование в DataFrame. \"\"\"\n",
    "    tree = ET.parse(tmx_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    data = []\n",
    "    for tu in root.findall('.//tu'):\n",
    "        tuv = tu.findall('tuv')\n",
    "        if len(tuv) >= 2:\n",
    "            src_text = tuv[0].find('seg').text\n",
    "            trg_text = tuv[1].find('seg').text\n",
    "            data.append({'source': src_text, 'target': trg_text})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования:\n",
    "url = 'https://object.pouta.csc.fi/OPUS-wikimedia/v20230407/tmx/en-ru.tmx.gz'  # Замените на вашу ссылку\n",
    "\n",
    "# Укажите URL архива и локальные пути для сохранения\n",
    "download_path = os.path.join(DATA_PATH, 'en-ru.tmx.gz')\n",
    "tmx_path = os.path.join(DATA_PATH, 'en-ru.tmx')\n",
    "\n",
    "if download_and_unpack(url, download_path):\n",
    "    extract_gz(download_path, tmx_path)\n",
    "    dataframe = read_tmx_to_dataframe(tmx_path)\n",
    "    print(dataframe.head())\n",
    "\n",
    "    # Опционально: удаление временных файлов\n",
    "    # os.remove(download_path)\n",
    "    # os.remove(tmx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmx_path = os.path.join(DATA_PATH, 'en-ru.tmx')\n",
    "dataframe = read_tmx_to_dataframe(tmx_path)\n",
    "dataframe = dataframe[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка данных от пустых строк и None\n",
    "dataframe = dataframe.dropna()  # Удаление строк, где есть хотя бы один None\n",
    "dataframe = dataframe[dataframe['source'].str.strip().astype(bool) & dataframe['target'].str.strip().astype(bool)]  # Удаление пустых строк\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет\n",
    "## Токенизация и создание словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, lang, max_vocab_size=10000, min_freq=1, max_length=100):\n",
    "        self.tokenizer = spacy.load(lang)\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_length = max_length\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return ['<bos>'] +[token.text.lower() for token in self.tokenizer(text)] + ['<eos>']\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        token_stream = (self._tokenize(sentence) for sentence in tqdm(data, desc=\"Vocab build\"))\n",
    "        \n",
    "        self.vocab = build_vocab_from_iterator(token_stream, max_tokens=self.max_vocab_size, specials=self.specials, special_first=True)\n",
    "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokenized_text = self._tokenize(text)[:self.max_length]\n",
    "        return [self.vocab[token] for token in tokenized_text]\n",
    "\n",
    "    def detokenize(self, numericalized_text):\n",
    "        decoded_text = [self.vocab.get_itos()[index] \\\n",
    "            for index in numericalized_text \\\n",
    "                if self.vocab.get_itos()[index] not in self.specials]\n",
    "        return ' '.join(decoded_text)\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataframe: pd.DataFrame, \n",
    "                 src_col: str, \n",
    "                 trg_col: str, \n",
    "                 tokenizer_src: TextTokenizer, \n",
    "                 tokenizer_trg: TextTokenizer, \n",
    "                 padding_value=1):\n",
    "        self.dataframe = dataframe\n",
    "        self.src_col = src_col\n",
    "        self.trg_col = trg_col\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_trg = tokenizer_trg\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.dataframe.iloc[idx][self.src_col]\n",
    "        trg_sentence = self.dataframe.iloc[idx][self.trg_col]\n",
    "        tokenized_src = self.tokenizer_src.tokenize(src_sentence)\n",
    "        tokenized_trg = self.tokenizer_trg.tokenize(trg_sentence)\n",
    "        return {\n",
    "            \"src\": torch.tensor(tokenized_src, dtype=torch.long),\n",
    "            \"trg\": torch.tensor(tokenized_trg, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        src_batch = [item['src'] for item in batch]\n",
    "        trg_batch = [item['trg'] for item in batch]\n",
    "        src_padded = pad_sequence(src_batch, padding_value=self.padding_value, batch_first=True)\n",
    "        trg_padded = pad_sequence(trg_batch, padding_value=self.padding_value, batch_first=True)\n",
    "        return {\"src\": src_padded, \"trg\": trg_padded}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = {'source': [\"Hello world\", \"How are you?\"], 'target': [\"Привет мир\", \"Как у тебя дела?\"]}\n",
    "# # df = pd.DataFrame(data)\n",
    "\n",
    "# # Создание токенизаторов и построение словарей\n",
    "# tokenizer_src = TextTokenizer(lang='en_core_web_sm')\n",
    "# tokenizer_trg = TextTokenizer(lang='ru_core_news_sm')\n",
    "# tokenizer_src.build_vocab(dataframe['source'])\n",
    "# tokenizer_trg.build_vocab(dataframe['target'])\n",
    "\n",
    "# # Разделение данных на обучающие и тестовые наборы\n",
    "# # train_df, val_df = train_test_split(dataframe, test_size=test_size, random_state=42)\n",
    "\n",
    "# # Пример использования\n",
    "# sample_text = \"Hello world\"\n",
    "# numericalized = tokenizer_src.tokenize(sample_text)\n",
    "# print(\"Numericalized:\", numericalized)\n",
    "# print(\"Detokenized:\", tokenizer_src.detokenize(numericalized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TranslationDataset(dataframe, 'source', 'target', tokenizer_src, tokenizer_trg)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# for bath in tqdm(dataloader, desc=\"Testing dataloader\"):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для реализации механизма Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout_rate=0.1, scale_factor=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size  # Размерность эмбеддинга\n",
    "        self.heads = heads  # Количество attention heads\n",
    "        self.head_dim = embed_size // heads  # Размерность для каждой головы внимания\n",
    "        self.scale_factor = scale_factor if scale_factor is not None \\\n",
    "            else (self.embed_size ** 0.5)\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size должен быть кратен количеству голов\"\n",
    "\n",
    "        # Инициализация весов для query, key и value\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # Объединение результатов голов внимания\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        # Получение размера пакета\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Разбиваем входные тензоры на головы\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # Применяем линейные преобразования к значениям\n",
    "        keys = self.keys(keys)  # Применяем линейные преобразования к ключам\n",
    "        queries = self.queries(queries)  # Применяем линейные преобразования к запросам\n",
    "\n",
    "        # Multiplying queries and keys for attention scores (N, heads, query_len, key_len)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalizing the attention scores using softmax\n",
    "        attention = torch.softmax(energy / self.scale_factor, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Multiplying the attention scores with the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            batch_size, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "                \n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для одного блока Transformer, включающий в себя Self-Attention и Feed Forward сеть.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)  # Создаем слой Self-Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Слой нормализации для residual connection после attention\n",
    "        self.norm2 = nn.LayerNorm(embed_size)  # Слой нормализации для residual connection после feed forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),  # Линейный слой\n",
    "            nn.ReLU(),  # Функция активации ReLU\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Линейный слой\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Слой dropout для регуляризации\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через блок Transformer.\n",
    "\n",
    "        Аргументы:\n",
    "            value: Тензор значений размером (batch_size, seq_length, embed_size)\n",
    "            key: Тензор ключей размером (batch_size, seq_length, embed_size)\n",
    "            query: Тензор запросов размером (batch_size, seq_length, embed_size)\n",
    "            mask: Маска внимания для исключения паддинга, размером (batch_size, 1, seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через блок Transformer\n",
    "        \"\"\"\n",
    "        # Проходим через механизм Self-Attention\n",
    "        attention_out, _ = self.attention(value, key, query, mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        x = self.dropout(self.norm1(attention_out + query))\n",
    "        # Проходим через feed forward сеть\n",
    "        forward_out = self.feed_forward(x)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        out = self.dropout(self.norm2(forward_out + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для Encoder части Transformer, содержащий стек Transformer блоков.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 embed_size, \n",
    "                 num_layers, \n",
    "                 heads, \n",
    "                 device, \n",
    "                 forward_expansion, \n",
    "                 dropout, \n",
    "                 max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # Создаем эмбеддинги слов\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)  # Создаем позиционные эмбеддинги\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size, \n",
    "                    heads, \n",
    "                    dropout, \n",
    "                    forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )  # Создаем стек блоков Transformer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через стек блоков Transformer Encoder.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, src_seq_length)\n",
    "            mask: Маска внимания для исключения паддинга, размером (batch_size, 1, src_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через стек блоков Transformer Encoder\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)  # Генерируем позиции\n",
    "        \n",
    "        # Получаем эмбеддинги слов и позиций и суммируем их\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Проходим через стек блоков Transformer\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для блока Decoder, который включает в себя Self-Attention, Encoder-Decoder Attention и Feed Forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)  # Создаем слой Self-Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.encoder_decoder_attention = SelfAttention(embed_size, heads)  # Создаем слой внимания между Decoder и Encoder\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),  # Линейный слой\n",
    "            nn.ReLU(),  # Функция активации ReLU\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),  # Линейный слой\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Слой dropout для регуляризации\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через блок Decoder.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, trg_seq_length, embed_size)\n",
    "            value: Тензор значений размером (batch_size, src_seq_length, embed_size)\n",
    "            key: Тензор ключей размером (batch_size, src_seq_length, embed_size)\n",
    "            src_mask: Маска внимания для исключения паддинга в Encoder, размером (batch_size, 1, src_seq_length)\n",
    "            trg_mask: Маска внимания для исключения предсказаний из будущего, размером (batch_size, trg_seq_length, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через блок Decoder\n",
    "        \"\"\"\n",
    "        # Проходим через механизм Self-Attention внутри Decoder\n",
    "        attention_out, _ = self.attention(x, x, x, trg_mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        query = self.dropout(self.norm1(attention_out + x))\n",
    "        \n",
    "        # Проходим через механизм внимания между Decoder и Encoder\n",
    "        encoder_decoder_attention_out, _ = self.encoder_decoder_attention(value, key, query, src_mask)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        query = self.dropout(self.norm2(encoder_decoder_attention_out + query))\n",
    "\n",
    "        # Проходим через feed forward сеть\n",
    "        forward_out = self.feed_forward(query)\n",
    "        # Применяем residual connection и нормализацию\n",
    "        out = self.dropout(self.norm3(forward_out + query))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс для Decoder части Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        trg_vocab_size, \n",
    "        embed_size, \n",
    "        num_layers, \n",
    "        heads, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        device, \n",
    "        max_length):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)  # Создаем эмбеддинги слов\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)  # Создаем позиционные эмбеддинги\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "            for _ in range(num_layers)\n",
    "        ])  # Создаем стек блоков Decoder\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)  # Финальный линейный слой для предсказания слов\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Прохождение входных данных через Decoder модель Transformer.\n",
    "\n",
    "        Аргументы:\n",
    "            x: Тензор входных данных размером (batch_size, trg_seq_length)\n",
    "            enc_out: Выходные данные Encoder размером (batch_size, src_seq_length, embed_size)\n",
    "            src_mask: Маска внимания для исключения паддинга в Encoder, размером (batch_size, 1, src_seq_length)\n",
    "            trg_mask: Маска внимания для исключения предсказаний из будущего, размером (batch_size, trg_seq_length, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            out: Результат прохождения данных через Decoder модель Transformer\n",
    "        \"\"\"\n",
    "        batch_size, trg_seq_length = x.shape\n",
    "        positions = torch.arange(0, trg_seq_length).expand(batch_size, trg_seq_length).to(self.device)  # Генерируем позиции\n",
    "\n",
    "        # Получаем эмбеддинги слов и позиций и суммируем их\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        # Проходим через стек блоков Decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)  # Применяем линейный слой для предсказания следующего слова\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс, интегрирующий Encoder и Decoder в полную модель Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 trg_vocab_size, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 embed_size=256, \n",
    "                 num_layers=6, \n",
    "                 forward_expansion=4, \n",
    "                 heads=8, \n",
    "                 dropout=0, \n",
    "                 device=\"cuda\", \n",
    "                 max_length=100):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length\n",
    "        )\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        Создание маски для исключения влияния padding токенов в процессе внимания.\n",
    "\n",
    "        Аргументы:\n",
    "            src: Тензор исходных данных размером (batch_size, src_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            src_mask: Маска для исключения padding токенов размером (batch_size, 1, 1, src_seq_length)\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (batch_size, 1, 1, src_len) для работы с механизмом внимания\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.size()\n",
    "        no_peak_mask = torch.tril(torch.ones((1, trg_len, trg_len), device=self.device)).bool()\n",
    "        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_mask = no_peak_mask & pad_mask\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Полный проход через модель.\n",
    "\n",
    "        Аргументы:\n",
    "            src: Тензор исходных данных размером (batch_size, src_seq_length)\n",
    "            trg: Тензор целевых данных размером (batch_size, trg_seq_length)\n",
    "\n",
    "        Возвращает:\n",
    "            output: Результат предсказания модели размером (batch_size, trg_seq_length, trg_vocab_size)\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    src_vocab_size = 32000  # Предполагаем, что у нас есть 32K уникальных токенов для исходного языка\n",
    "    trg_vocab_size = 32000  # и для целевого языка\n",
    "    embed_size = 512        # Размер эмбеддингов\n",
    "    num_layers = 6          # Количество слоев в энкодере и декодере\n",
    "    forward_expansion = 4   # Коэффициент увеличения для полносвязного слоя\n",
    "    heads = 8               # Количество голов в механизме многослойного внимания\n",
    "    dropout = 0.1           # Вероятность dropout\n",
    "    max_length = 100        # Максимальная длина последовательности\n",
    "    min_freq = 2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')         # Устройство для тренировки: 'cuda' или 'cpu'\n",
    "    learning_rate = 0.0005  # Скорость обучения\n",
    "    batch_size = 64         # Размер батча для обучения\n",
    "    clip = 1\n",
    "    num_epochs = 10\n",
    "    test_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, \n",
    "                 tokenizer_src: TextTokenizer, \n",
    "                 tokenizer_trg: TextTokenizer,\n",
    "                 weights={'bleu': 0.34, 'rouge': 0.33, 'meteor': 0.33}):\n",
    "        \"\"\"\n",
    "        Инициализация Evaluator с токенизаторами для исходного и целевого языков.\n",
    "\n",
    "        :param tokenizer_src: Токенизатор для исходного языка.\n",
    "        :param tokenizer_trg: Токенизатор для целевого языка.\n",
    "        :param weights: Веса.\n",
    "        \"\"\"\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_trg = tokenizer_trg\n",
    "        self.weights = weights        \n",
    "        self.rouge = Rouge()\n",
    "\n",
    "    def evaluate(self, model, dataloader):\n",
    "        \"\"\"\n",
    "        Оценивает модель на данных, загруженных через data_loader.\n",
    "\n",
    "        :param model: Модель для оценки.\n",
    "        :param data_loader: DataLoader, предоставляющий данные для оценки.\n",
    "        :return: Словарь с результатами по метрикам.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                src  = batch['src']\n",
    "                trg  = batch['trg']\n",
    "\n",
    "                # Генерация вывода модели\n",
    "                output = model(src, trg[:, :-1])  # Исключаем токен <eos>\n",
    "                output = output.argmax(-1)\n",
    "                \n",
    "                # Детокенизация результата\n",
    "                for true, pred in zip(trg, output):\n",
    "                    true_sentence = self.tokenizer_trg.detokenize(true.numpy())\n",
    "                    pred_sentence = self.tokenizer_trg.detokenize(pred.numpy())\n",
    "                    references.append([true_sentence])\n",
    "                    hypotheses.append(pred_sentence)\n",
    "\n",
    "        # Вычисление метрик BLEU, ROUGE, METEOR\n",
    "        bleu_score = corpus_bleu(references, hypotheses)\n",
    "        # rouge_scores = self.rouge.get_scores(hypotheses, [' '.join(ref[0]) for ref in references], avg=True)\n",
    "        # rouge_score = rouge_scores['rouge-l']['f']\n",
    "        rouge_score = 0\n",
    "        # list_meteor_score = [meteor_score([ref[0]], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "        # avg_meteor_score = np.mean(list_meteor_score)\n",
    "        avg_meteor_score = 0\n",
    "\n",
    "        # Словарь с результатами\n",
    "        results = {\n",
    "            'overall': self.weights['bleu'] * bleu_score +\n",
    "                       self.weights['rouge'] * rouge_score +\n",
    "                       self.weights['meteor'] * avg_meteor_score,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge': rouge_score,\n",
    "            'meteor': avg_meteor_score\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def evaluate_on_indices(self, model, dataloader):\n",
    "        \"\"\"\n",
    "        Оценивает модель на данных, используя индексы токенов для вычисления BLEU.\n",
    "\n",
    "        :param model: Модель для оценки.\n",
    "        :param data_loader: DataLoader, предоставляющий данные для оценки.\n",
    "        :return: Словарь с результатами метрики BLEU.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                src  = batch['src']\n",
    "                trg  = batch['trg']\n",
    "                \n",
    "                # Генерация вывода модели\n",
    "                output = model(src, trg[:, :-1])\n",
    "                output = output.argmax(-1)\n",
    "                \n",
    "                # Сохранение индексов без детокенизации\n",
    "                for true, pred in zip(trg, output):\n",
    "                    # Убираем индексы для специальных токенов\n",
    "                    true_indices = [token for token in true.numpy() \\\n",
    "                        if token not in [self.tokenizer_trg.vocab[spec] \\\n",
    "                            for spec in self.tokenizer_trg.specials]]\n",
    "                    pred_indices = [token for token in pred.numpy() \\\n",
    "                        if token not in [self.tokenizer_trg.vocab[spec] \\\n",
    "                            for spec in self.tokenizer_trg.specials]]\n",
    "                    \n",
    "                    references.append([true_indices])\n",
    "                    hypotheses.append(pred_indices)\n",
    "\n",
    "        # Вычисление метрик BLEU, ROUGE, METEOR\n",
    "        bleu_score = corpus_bleu(references, hypotheses)\n",
    "        rouge_score = self.rouge.get_scores(hypotheses, references, avg=True)['f']\n",
    "        # list_meteor_score = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "        # avg_meteor_score = np.mean(list_meteor_score)\n",
    "        avg_meteor_score = 0\n",
    "\n",
    "        # Словарь с результатами\n",
    "        results = {\n",
    "            'overall': self.weights['bleu'] * bleu_score +\n",
    "                       self.weights['rouge'] * rouge_score +\n",
    "                       self.weights['meteor'] * avg_meteor_score,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge': rouge_score,\n",
    "            'meteor': avg_meteor_score\n",
    "        }\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 model:Transformer, \n",
    "                 evaluator: Evaluator, \n",
    "                 optimizer:nn.CrossEntropyLoss,\n",
    "                 criterion:optim.Adam,\n",
    "                 clip=1,\n",
    "                 device='cuda',\n",
    "                 printing_period=None,\n",
    "                 imgs_path=Path(''),\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Инициализация Trainer для модели Transformer.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.evaluator = evaluator\n",
    "        self.clip = clip\n",
    "        \n",
    "        self.printing_period = printing_period\n",
    "        self.imgs_path = imgs_path\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.metrics = []\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train(self, train_dataloader):\n",
    "        \"\"\"\n",
    "        Обучение модели\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            src: Tensor = batch['src'].to(self.device)\n",
    "            trg: Tensor = batch['trg'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output: Tensor = self.model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(output, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Аккумулируем потери\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Вычисляем среднюю потерю за эпоху\n",
    "        avg_loss = train_loss / len(train_dataloader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "\n",
    "    def eval(self, val_dataloader):\n",
    "        \"\"\"\n",
    "        Валидация модели для оценки производительности на валидационном наборе данных.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                src: Tensor= batch['src'].to(self.device)\n",
    "                trg: Tensor = batch['trg'].to(self.device)     \n",
    "                           \n",
    "                output: Tensor = self.model(src, trg[:, :-1])\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output.contiguous().view(-1, output_dim)\n",
    "                trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(output, trg)\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "        avg_loss = validation_loss / len(val_dataloader)\n",
    "        self.val_losses.append(avg_loss)\n",
    "        \n",
    "        metric_scores = self.evaluator.evaluate(self.model, val_dataloader)\n",
    "        self.metrics.append(metric_scores)\n",
    "        \n",
    "        return avg_loss, metric_scores\n",
    "        \n",
    "    def fit(self, num_epochs, train_dataloader, val_dataloader):\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = self.train(train_dataloader)\n",
    "            val_epoch_loss, metric_scores = self.eval(val_dataloader)   \n",
    "                 \n",
    "            if self.printing_period and (epoch + 1) % self.printing_period == 0:\n",
    "                print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Overall: {metric_scores[\"overall\"]:.4f}')\n",
    "                \n",
    "        return val_epoch_loss, metric_scores\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Сохранение обученной модели.\n",
    "\n",
    "        Параметры:\n",
    "            path: путь для сохранения модели.\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Загрузка модели из файла.\n",
    "\n",
    "        Параметры:\n",
    "            path: путь к файлу с сохраненной моделью.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def _save_fig(self, fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "        path = self.imgs_path / f\"{fig_id}.{fig_extension}\"\n",
    "        if tight_layout:\n",
    "            plt.tight_layout()\n",
    "        plt.savefig(path, format=fig_extension, dpi=resolution) \n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs_range = range(1, len(self.val_losses) + 1)\n",
    "        fig = plt.figure(figsize=(15, 6))  # Устанавливаем размер фигуры\n",
    "        \n",
    "        ax11 = plt.subplot(2, 2, 1)\n",
    "        ax11.plot(epochs_range, self.train_losses, label='Train Loss', color='tab:red')\n",
    "        ax11.plot(epochs_range, self.val_losses, label='Validation Loss', color='tab:blue')\n",
    "        ax11.set_title('Losses over Epochs')\n",
    "        ax11.set_xlabel('Epochs')\n",
    "        ax11.set_ylabel('Loss')\n",
    "        # ax11.tick_params(axis='y', labelcolor='tab:red')\n",
    "        ax11.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax11.legend(loc='upper right')\n",
    "\n",
    "        ax12 = plt.subplot(2, 2, 2)\n",
    "        ax12.plot(epochs_range, [m['overall'] for m in self.metrics], label='Overall Score', color='tab:green')\n",
    "        ax12.set_title('Overall Evaluation Score')\n",
    "        ax12.set_xlabel('Epochs')\n",
    "        ax12.set_ylabel('Score')\n",
    "        ax12.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax12.legend(loc='upper right')\n",
    "\n",
    "        # Отдельные графики для каждой метрики\n",
    "        ax21 = plt.subplot(2, 2, 3)\n",
    "        ax21.plot(epochs_range, [m['bleu'] for m in self.metrics], label='BLEU Score', color='tab:red')\n",
    "        ax21.plot(epochs_range, [m['rouge'] for m in self.metrics], label='ROUGE Score', color='tab:pink')\n",
    "        ax21.plot(epochs_range, [m['meteor'] for m in self.metrics], label='METEOR Score', color='tab:brown')\n",
    "        ax21.set_title('Individual Metrics')\n",
    "        ax21.set_xlabel('Epochs')\n",
    "        ax21.set_ylabel('Score')\n",
    "        ax21.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax21.legend(loc='upper right')      \n",
    "\n",
    "        fig.tight_layout()  # Убедимся, что макет не нарушен\n",
    "        self._save_fig(\"train_metrics\")  # extra code\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "class TrainingManager:\n",
    "    def __init__(self, \n",
    "                 dataframe: pd.DataFrame,\n",
    "                 features: str, \n",
    "                 targets: str,\n",
    "                 config: Config,\n",
    "                 data_path=DATA_PATH,\n",
    "                 imgs_path=IMAGES_PATH):\n",
    "        \"\"\"\n",
    "        Инициализация менеджера обучения.        \n",
    "        :param features: Признаки для обучения модели.\n",
    "        :param targets: Целевые значения.\n",
    "        :param model_params: Параметры модели, включая размеры слоёв и функцию активации.\n",
    "        :param train_params: Параметры обучения, включая размер батча, количество эпох и скорость обучения.\n",
    "        \"\"\"\n",
    "        self.uniq_name = '_'.join(features + targets)\n",
    "        self.data_path = data_path  \n",
    "        self.imgs_path = imgs_path    \n",
    "        \n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.tokenizer_src = TextTokenizer(\n",
    "            lang='en_core_web_sm',\n",
    "            max_vocab_size=config.src_vocab_size,\n",
    "            min_freq=config.min_freq,\n",
    "            max_length=config.max_length    \n",
    "        )\n",
    "        self.tokenizer_trg = TextTokenizer(\n",
    "            lang='ru_core_news_sm',\n",
    "            max_vocab_size=config.trg_vocab_size,\n",
    "            min_freq=config.min_freq,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        self.tokenizer_src.build_vocab(dataframe['source'])\n",
    "        self.tokenizer_trg.build_vocab(dataframe['target'])\n",
    "\n",
    "        train_df, val_df = train_test_split(dataframe, test_size=config.test_size, random_state=42)\n",
    "\n",
    "        train_dataset = TranslationDataset(\n",
    "            dataframe=train_df,\n",
    "            src_col='source',\n",
    "            trg_col='target',\n",
    "            tokenizer_src=self.tokenizer_src,\n",
    "            tokenizer_trg=self.tokenizer_trg,\n",
    "            padding_value=self.tokenizer_src.vocab['<pad>'],\n",
    "        )\n",
    "\n",
    "        val_dataset = TranslationDataset(\n",
    "            dataframe=val_df,\n",
    "            src_col='source',\n",
    "            trg_col='target',\n",
    "            tokenizer_src=self.tokenizer_src,\n",
    "            tokenizer_trg=self.tokenizer_trg,\n",
    "            padding_value=self.tokenizer_src.vocab['<pad>'],\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            collate_fn=train_dataset.collate_fn, \n",
    "            shuffle=True\n",
    "        )\n",
    "        self.val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            collate_fn=val_dataset.collate_fn, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Шаг 1: Определение модели\n",
    "        model = Transformer(\n",
    "            src_vocab_size=len(self.tokenizer_src.vocab),\n",
    "            trg_vocab_size=len(self.tokenizer_trg.vocab),\n",
    "            src_pad_idx=self.tokenizer_src.vocab['<pad>'],\n",
    "            trg_pad_idx=self.tokenizer_trg.vocab['<pad>'],\n",
    "            embed_size=config.embed_size,\n",
    "            num_layers=config.num_layers,\n",
    "            forward_expansion=config.forward_expansion,\n",
    "            heads=config.heads,\n",
    "            dropout=config.dropout,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        # Шаг 2: Определение функции потерь\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer_src.vocab['<pad>'])\n",
    "\n",
    "        # Шаг 3: Определение оптимизатора\n",
    "        optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "        \n",
    "        evaluator = Evaluator(\n",
    "            self.tokenizer_src,\n",
    "            self.tokenizer_trg,    \n",
    "        )\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model,\n",
    "            evaluator,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            clip=config.clip,\n",
    "            device=config.device,\n",
    "            printing_period=1,\n",
    "            imgs_path=self.imgs_path    \n",
    "        )  \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Запуск процесса обучения.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.trainer.fit(self.config.num_epochs, self.train_dataloader, self.val_dataloader)\n",
    "        self.trainer.plot_metrics()\n",
    "\n",
    "    # def predict(self, input, transform=True):\n",
    "    #     \"\"\"\n",
    "    #     Выполнение предсказаний с помощью обученной модели.\n",
    "    #     \"\"\"\n",
    "    #     features = input\n",
    "    #     if transform:\n",
    "    #         features = self.scaler_features.transform(input)\n",
    "    #     else:\n",
    "    #         features = input\n",
    "    #     scaled_predictions = self.trainer.predict(features)\n",
    "    #     return self.scaler_targets.inverse_transform(scaled_predictions)\n",
    "\n",
    "    # def save(self):\n",
    "    #     self._save_model()\n",
    "    #     self._save_standard_scalar()\n",
    " \n",
    "    # def load(self):\n",
    "    #     self._load_model()\n",
    "    #     self._load_standard_scalar()\n",
    "    \n",
    "    # def _save_model(self):\n",
    "    #     \"\"\"\n",
    "    #     Сохранение обученной модели.        \n",
    "    #     \"\"\"\n",
    "    #     self.trainer.save_model(self.data_path+self.uniq_name+'_model_weight.pth')\n",
    "        \n",
    "    # def _load_model(self):\n",
    "    #     \"\"\"\n",
    "    #     Загрузка обученной модели.        \n",
    "    #     \"\"\"\n",
    "    #     self.trainer = Trainer.load_model(self.data_path+self.uniq_name+'_model_weight.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config\n",
    "\n",
    "config.src_vocab_size = 20000  # Предполагаем, что у нас есть 32K уникальных токенов для исходного языка\n",
    "config.trg_vocab_size = 20000  # и для целевого языка\n",
    "config.embed_size = 64         # Размер эмбеддингов\n",
    "config.num_layers = 6          # Количество слоев в энкодере и декодере\n",
    "config.forward_expansion = 4   # Коэффициент увеличения для полносвязного слоя\n",
    "config.heads = 8               # Количество голов в механизме многослойного внимания\n",
    "config.dropout = 0.1           # Вероятность dropout\n",
    "config.min_freq = 2\n",
    "config.max_length = 100        # Максимальная длина последовательности\n",
    "config.device = \"cpu\"          # Устройство для тренировки: 'cuda' или 'cpu'\n",
    "config.learning_rate = 0.005  # Скорость обучения\n",
    "config.batch_size = 128          # Размер батча для обучения\n",
    "config.clip = 1                # \n",
    "config.num_epochs = 25\n",
    "config.test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "# from tqdm.auto import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # Определение гиперпараметров\n",
    "# max_vocab_size = 10000\n",
    "# min_freq = 2\n",
    "# max_length = 80\n",
    "# embed_size = 64\n",
    "# num_layers = 3\n",
    "# forward_expansion = 2\n",
    "# heads = 4\n",
    "# dropout = 0.1\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 256\n",
    "# num_epochs = 10\n",
    "# clip = 1\n",
    "# test_size = 0.1\n",
    "# device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Создание токенизаторов и построение словарей\n",
    "# tokenizer_src = TextTokenizer(\n",
    "#     lang='en_core_web_sm',\n",
    "#     max_vocab_size=max_vocab_size,\n",
    "#     min_freq=min_freq,\n",
    "#     max_length=max_length    \n",
    "# )\n",
    "# tokenizer_trg = TextTokenizer(\n",
    "#     lang='ru_core_news_sm',\n",
    "#     max_vocab_size=max_vocab_size,\n",
    "#     min_freq=min_freq,\n",
    "#     max_length=max_length\n",
    "# )\n",
    "# tokenizer_src.build_vocab(dataframe['source'])\n",
    "# tokenizer_trg.build_vocab(dataframe['target'])\n",
    "\n",
    "# train_df, val_df = train_test_split(dataframe, test_size=test_size, random_state=42)\n",
    "\n",
    "# train_dataset = TranslationDataset(\n",
    "#     dataframe=train_df,\n",
    "#     src_col='source',\n",
    "#     trg_col='target',\n",
    "#     tokenizer_src=tokenizer_src,\n",
    "#     tokenizer_trg=tokenizer_trg,\n",
    "#     padding_value=tokenizer_src.vocab['<pad>'],\n",
    "# )\n",
    "\n",
    "# val_dataset = TranslationDataset(\n",
    "#     dataframe=val_df,\n",
    "#     src_col='source',\n",
    "#     trg_col='target',\n",
    "#     tokenizer_src=tokenizer_src,\n",
    "#     tokenizer_trg=tokenizer_trg,\n",
    "#     padding_value=tokenizer_src.vocab['<pad>'],\n",
    "# )\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=batch_size, \n",
    "#     collate_fn=train_dataset.collate_fn, \n",
    "#     shuffle=True\n",
    "# )\n",
    "# val_dataloader = DataLoader(\n",
    "#     val_dataset, \n",
    "#     batch_size=batch_size, \n",
    "#     collate_fn=val_dataset.collate_fn, \n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # Шаг 1: Определение модели\n",
    "# model = Transformer(\n",
    "#     src_vocab_size=len(tokenizer_src.vocab),\n",
    "#     trg_vocab_size=len(tokenizer_trg.vocab),\n",
    "#     src_pad_idx=tokenizer_src.vocab['<pad>'],\n",
    "#     trg_pad_idx=tokenizer_trg.vocab['<pad>'],\n",
    "#     embed_size=embed_size,\n",
    "#     num_layers=num_layers,\n",
    "#     forward_expansion=forward_expansion,\n",
    "#     heads=heads,\n",
    "#     dropout=dropout,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# # Шаг 2: Определение функции потерь\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_src.vocab['<pad>'])\n",
    "\n",
    "# # Шаг 3: Определение оптимизатора\n",
    "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# evaluator = Evaluator(\n",
    "#     tokenizer_src,\n",
    "#     tokenizer_trg,    \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Шаг 4: Обучение модели\n",
    "# def train(model: Transformer, iterator, optimizer: Adam, criterion: nn.CrossEntropyLoss, clip):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     for batch in tqdm(iterator, desc=\"Training\"):\n",
    "#         src = batch[\"src\"]\n",
    "#         trg = batch[\"trg\"]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(src, trg[:, :-1])\n",
    "#         output_dim = output.shape[-1]\n",
    "\n",
    "#         output = output.contiguous().view(-1, output_dim)\n",
    "#         trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "#         loss = criterion(output, trg)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(iterator)\n",
    "\n",
    "# def evaluate(model, iterator, criterion):\n",
    "#     model.eval()\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(iterator, desc=\"Evaluation\"):\n",
    "#             src = batch[\"src\"]\n",
    "#             trg = batch[\"trg\"]\n",
    "\n",
    "#             output = model(src, trg[:, :-1])\n",
    "#             output_dim = output.shape[-1]\n",
    "\n",
    "#             output = output.contiguous().view(-1, output_dim)\n",
    "#             trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "#             loss = criterion(output, trg)\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(iterator)\n",
    "\n",
    "# CLIP = 1\n",
    "\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "#     valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'transformer_model.pt')\n",
    "\n",
    "#     print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     evaluator,\n",
    "#     optimizer,\n",
    "#     criterion,\n",
    "#     clip=clip,\n",
    "#     device=device,\n",
    "#     printing_period=1,\n",
    "#     imgs_path=IMAGES_PATH    \n",
    "# )\n",
    "# trainer.fit(\n",
    "#     num_epochs=num_epochs,\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     val_dataloader=val_dataloader,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_manager = TrainingManager(\n",
    "    dataframe,\n",
    "    'source',\n",
    "    'target',\n",
    "    config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_manager.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выполнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
